{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"CNN_textClassifier.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ymWh_PgHwjt3","colab_type":"text"},"source":["<center> <div style=\"text-align: center\"> <h1>DNN based Text Classifier using Keras   </h1></div>\n","<hl></center>"]},{"cell_type":"markdown","metadata":{"id":"A4n2GRLJwjt6","colab_type":"text"},"source":["**Anoop K. & Manjary P. Gangan** <br>\n","CIDA Labs, Department of Computer Science<br>\n","University of Calicut<br>\n","https://dcs.uoc.ac.in/~anoop <br>\n","https://dcs.uoc.ac.in/~manjary <br><br>\n","________________________"]},{"cell_type":"markdown","metadata":{"id":"wqbLdqs_wjt9","colab_type":"text"},"source":["<center><img width=\"600\" height=\"350\" src=\"https://drive.google.com/uc?id=1TD138tj4cb89_3E62wqsvqR9WUiwNDx_\"></center>"]},{"cell_type":"markdown","metadata":{"id":"etj1zQnWwjt_","colab_type":"text"},"source":["# Sentiment Analysis: Predict Sentiment from Movie Reviews\n","<div style=\"text-align: justify\">Definition 1: Sentiment analysis (also known as opinion mining or emotion AI) refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. </div> <br>\n","<div style=\"text-align: justify\"> Definition 2: Sentiment analysis is a type of data mining that measures the inclination of people’s opinions through natural language processing (NLP), computational linguistics and text analysis, which are used to extract and analyze subjective information from the Web - mostly social media and similar sources. The analyzed data quantifies the general public's sentiments or reactions toward certain products, people or ideas and reveal the contextual polarity of the information.</div>"]},{"cell_type":"markdown","metadata":{"id":"zjnoBqfuwjuC","colab_type":"text"},"source":["# Text to Vector\n","### 1. TF-IDF\n","In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n","<center>\n","<img width=\"600\" height=\"500\" src=\"https://drive.google.com/uc?id=18BFjUHxONPZiNT7wogyjA9KRWbiU3RjD\"> </center>\n","\n","### 2. One-Hot encoding\n","### 3. Word Embedding"]},{"cell_type":"markdown","metadata":{"id":"mThOJLiXwjuE","colab_type":"text"},"source":["# IMDB Movie Review Sentiment Problem Description\n","## Dataset Description : Large Movie Review Dataset v1.0\n","<div style=\"text-align: justify\">The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly polar moving reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given moving review has a positive or negative sentiment.\n","The data was collected by Stanford researchers and was used in a 2011 paper <a href=\"http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf\"> [PDF] </a>where a split of 50/50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n","The data was also used as the basis for a Kaggle competition titled “Bag of Words Meets Bags of Popcorn” in late 2014 to early 2015. Accuracy was achieved above 97% with winners achieving 99%.</div>\n"]},{"cell_type":"markdown","metadata":{"id":"VjxnaRTxwjuF","colab_type":"text"},"source":["## Load the IMDB Dataset With Keras\n","<div style=\"text-align: justify\">\n","Keras provides access to the IMDB dataset built-in. The keras.datasets.imdb.load_data() allows you to load the dataset in a format that is ready for use in neural network and deep learning models.The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of integers. More details; the reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".<br>\n","\n","Calling imdb.load_data() the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file.Usefully, the imdb.load_data() provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid the “the”‘s) and the maximum length of reviews to support. </div> <br>\n","**Note:** <br>\n","If you have raw text data, better to use keras <b>Text Preprocessing</b> package. \n","1. texts_to_sequences\n","2. text_to_word_sequence\n","3. one_hot <br>\n","etc."]},{"cell_type":"code","metadata":{"id":"86ZBpWDOyCKM","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ix9Y1_GTwjuL","colab_type":"code","outputId":"a096855e-b81b-470e-a265-439ab7cf41d0","executionInfo":{"status":"ok","timestamp":1591350649484,"user_tz":-330,"elapsed":8702,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["import numpy\n","from keras.datasets import imdb\n","from matplotlib import pyplot\n","# load the dataset\n","(X_train, y_train), (X_test, y_test) = imdb.load_data()\n","X = numpy.concatenate((X_train, X_test), axis=0)\n","y = numpy.concatenate((y_train, y_test), axis=0)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n","17465344/17464789 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PxIB0yHMwjuV","colab_type":"code","outputId":"7f24c825-f3cc-4e78-9bb1-11bb511c9048","executionInfo":{"status":"ok","timestamp":1591350651107,"user_tz":-330,"elapsed":1581,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# summarize size: Shape of training dataset\n","print(\"Total Data: \")\n","print(X.shape)\n","print(y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Total Data: \n","(50000,)\n","(50000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yuuhlFWBwjug","colab_type":"code","outputId":"e25939d3-be8b-4306-fc8d-fddf14dfdba7","executionInfo":{"status":"ok","timestamp":1591350655190,"user_tz":-330,"elapsed":1405,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Summarize number of classes\n","print(\"Classes: \")\n","print(numpy.unique(y))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Classes: \n","[0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6Vrk_Bb4wjuo","colab_type":"code","outputId":"3961c0d6-b402-47f0-f0d9-0e5189d81d6f","executionInfo":{"status":"ok","timestamp":1591350661763,"user_tz":-330,"elapsed":3375,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Summarize number of words\n","print(\"Number of words: \")\n","print(len(numpy.unique(numpy.hstack(X))))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of words: \n","88585\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zBkw1pOWwjuv","colab_type":"code","outputId":"2fdf7836-d39c-4a0d-afea-a91e40c5f801","executionInfo":{"status":"ok","timestamp":1591350669534,"user_tz":-330,"elapsed":1425,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# Summarize review length\n","print(\"Review length: \")\n","result = [len(x) for x in X]\n","print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n","#We can see that the average review has just under 300 words with a standard deviation of just over 200 words."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Review length: \n","Mean 234.76 words (172.911495)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j8Ns29VGwju3","colab_type":"text"},"source":["## Word Embeddings\n","<div style=\"text-align: justify\"> This is a technique where words are encoded as real-valued vectors in a high-dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks and deep learning models are we require numbers as input. </div>\n","<br><div style=\"text-align: justify\"> \n","Keras provides a convenient way to convert positive integer representations of words into a word embedding by an <b>Embedding layer</b>. The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an integer). The layer also allows you to specify the dimensionality for each word vector, called the output dimension.</div>\n","<br><div style=\"text-align: justify\"> \n","Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values. </div>\n","\n","imdb.load_data(nb_words=5000) <br>\n","X_train = sequence.pad_sequences(X_train, maxlen=500)<br>\n","X_test = sequence.pad_sequences(X_test, maxlen=500)<br>\n","Embedding(5000, 32, input_length=500)<br>\n","\n","The output of this first layer would be a matrix with the size 32×500 for a given review training or test pattern in integer format. <br>\n","\n","**Note:**\n","1. How to Use Word Embedding Layers for Deep Learning with Keras <br>\n","https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/"]},{"cell_type":"markdown","metadata":{"id":"yT4mlxmOwju4","colab_type":"text"},"source":["# Simple Multi-Layer Perceptron Model for the IMDB Dataset\n","## Load the Data"]},{"cell_type":"code","metadata":{"id":"oe9_9Y2Uwju6","colab_type":"code","colab":{}},"source":["# MLP for the IMDB problem\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","max_words = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVIZhN39wju-","colab_type":"text"},"source":["## Create the Model "]},{"cell_type":"code","metadata":{"id":"9YNMPzSOwju-","colab_type":"code","outputId":"2acf790d-c0c6-418c-f796-709b2601a145","executionInfo":{"status":"ok","timestamp":1591350685714,"user_tz":-330,"elapsed":1418,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":308}},"source":["# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, 32, input_length=max_words))\n","model.add(Flatten())\n","model.add(Dense(250, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 16000)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 250)               4000250   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 251       \n","=================================================================\n","Total params: 4,160,501\n","Trainable params: 4,160,501\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZpWjFk_SwjvE","colab_type":"text"},"source":["## Fit & Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"TJbq-g39wjvH","colab_type":"code","outputId":"b75c37a0-2ffb-439a-e1ec-9a28b7b55a64","executionInfo":{"status":"ok","timestamp":1591350834617,"user_tz":-330,"elapsed":129107,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/5\n","25000/25000 [==============================] - 25s 988us/step - loss: 0.3327 - accuracy: 0.8569 - val_loss: 0.2926 - val_accuracy: 0.8743\n","Epoch 2/5\n","25000/25000 [==============================] - 25s 988us/step - loss: 0.1370 - accuracy: 0.9505 - val_loss: 0.3254 - val_accuracy: 0.8719\n","Epoch 3/5\n","25000/25000 [==============================] - 25s 986us/step - loss: 0.0436 - accuracy: 0.9892 - val_loss: 0.4257 - val_accuracy: 0.8659\n","Epoch 4/5\n","25000/25000 [==============================] - 25s 983us/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 0.5114 - val_accuracy: 0.8654\n","Epoch 5/5\n","25000/25000 [==============================] - 24s 976us/step - loss: 0.0020 - accuracy: 0.9999 - val_loss: 0.5766 - val_accuracy: 0.8674\n","Accuracy: 86.74%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eWJzGxKZwjvM","colab_type":"text"},"source":["# One-Dimensional CNN Model for the IMDB Dataset\n","<b>Ref:</b> https://missinglink.ai/guides/keras/keras-conv1d-working-1d-convolutional-neural-networks-keras/\n","\n","\n","---\n","\n","\n","\n","\n","## Load the Data"]},{"cell_type":"code","metadata":{"id":"7VapZdJpwjvN","colab_type":"code","colab":{}},"source":["# CNN for the IMDB problem\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","# pad dataset to a maximum review length in words\n","max_words = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtgsL4Wl1wgp","colab_type":"code","outputId":"dccbf153-7913-4ef2-dbfd-713ce274bc03","executionInfo":{"status":"ok","timestamp":1591350852834,"user_tz":-330,"elapsed":1411,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["print('Shape of X train and X validation tensor:', X_train.shape,X_test.shape)\n","print('Shape of label train and validation tensor:', y_train.shape,y_test.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape of X train and X validation tensor: (25000, 500) (25000, 500)\n","Shape of label train and validation tensor: (25000,) (25000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rGnV3e2uwjvR","colab_type":"text"},"source":["## Create the Model\n","Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively."]},{"cell_type":"code","metadata":{"id":"dCKjQTlCwjvS","colab_type":"code","outputId":"6d29ec9c-7f54-4b87-d42c-1bb1ab05a0aa","executionInfo":{"status":"ok","timestamp":1591350856130,"user_tz":-330,"elapsed":1273,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, 32, input_length=max_words))\n","model.add(Conv1D(32, 3, padding='same', activation='relu'))\n","model.add(MaxPooling1D())\n","model.add(Flatten())\n","model.add(Dense(250, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n","_________________________________________________________________\n","max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n","_________________________________________________________________\n","flatten_2 (Flatten)          (None, 8000)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 250)               2000250   \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 251       \n","=================================================================\n","Total params: 2,163,605\n","Trainable params: 2,163,605\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3--w81LKwjvZ","colab_type":"text"},"source":["## Fit and Evaluate the Model"]},{"cell_type":"code","metadata":{"id":"Z0sYTH1vwjva","colab_type":"code","outputId":"05113f59-b9ee-429b-bf15-b73f0d59d4ff","executionInfo":{"status":"ok","timestamp":1591351002149,"user_tz":-330,"elapsed":133774,"user":{"displayName":"Anoop K","photoUrl":"","userId":"05252450542332776239"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["# Fit the model\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)\n","\n","# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/5\n","25000/25000 [==============================] - 26s 1ms/step - loss: 0.4226 - accuracy: 0.7901 - val_loss: 0.2742 - val_accuracy: 0.8864\n","Epoch 2/5\n","25000/25000 [==============================] - 26s 1ms/step - loss: 0.2132 - accuracy: 0.9174 - val_loss: 0.2812 - val_accuracy: 0.8826\n","Epoch 3/5\n","25000/25000 [==============================] - 25s 1ms/step - loss: 0.1623 - accuracy: 0.9386 - val_loss: 0.3007 - val_accuracy: 0.8805\n","Epoch 4/5\n","25000/25000 [==============================] - 25s 1ms/step - loss: 0.1162 - accuracy: 0.9604 - val_loss: 0.3361 - val_accuracy: 0.8760\n","Epoch 5/5\n","25000/25000 [==============================] - 25s 1ms/step - loss: 0.0714 - accuracy: 0.9795 - val_loss: 0.4054 - val_accuracy: 0.8731\n","Accuracy: 87.31%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nVdGIcoowjvd","colab_type":"text"},"source":["# CNN for Sentence Classification by Zhang et al. & Kim"]},{"cell_type":"markdown","metadata":{"id":"QyxEvDJOwjve","colab_type":"text"},"source":["**1. Zhang, Y., & Wallace, B. (2015), A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification.** </br>\n","**2. Kim, Yoon. \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882 (2014).** "]},{"cell_type":"markdown","metadata":{"id":"zv9FqCSWwjvf","colab_type":"text"},"source":["<center><img width=\"600\" height=\"500\" src=\"https://drive.google.com/uc?id=1Q90KCeWdyB2vACz2z9rQR27nssYO7Hxi\"> </center>\n","<div style=\"text-align: justify\">\n"," Illustration of a Convolutional Neural Network (CNN) architecture for sentence classification. Here we depict three filter region sizes: 2, 3 and 4, each of which has 2 filters. Every filter performs convolution on the sentence matrix and generates (variable-length) feature maps. Then 1-max pooling is performed over each map, i.e., the largest number from each feature map is recorded. Thus a univariate feature vector is generated from all six maps, and these 6 features are concatenated to form a feature vector for the penultimate layer. The final softmax layer then receives this feature vector as input and uses it to classify the sentence; here we assume binary classification and hence depict two possible output states.</div> \n","<h1> let's try to implement this model! </h1> "]},{"cell_type":"markdown","metadata":{"id":"7ENK6K0Kwjvi","colab_type":"text"},"source":["Reference: \n","1. https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n","2. https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/\n","3. http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/ <br>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FbUz-s5Pwjvr","colab_type":"text"},"source":["<center><img width=\"400\" height=\"250\" src=\"https://drive.google.com/uc?id=1LdciBzE4Oc__NE00Bw0TisofYTP0qGc0\"> </center>"]}]}